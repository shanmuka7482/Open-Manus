import base64
import json
import os
from io import BytesIO
from urllib.parse import urlparse, urlunparse

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
import requests
from pptx import Presentation
from pptx.util import Inches, Pt

from app.logger import logger
from app.services.agent_service import agent_service
from dotenv import load_dotenv

load_dotenv()

def generate_pptx_from_slides(slides_list, title="Presentation"):
    prs = Presentation()
    title_slide_layout = prs.slide_layouts[0]
    bullet_slide_layout = prs.slide_layouts[1]

    # Create title slide
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = title
    slide.placeholders[1].text = "Generated by OpenManus"

    # Create content slides
    for slide_text in slides_list:
        lines = slide_text.split("\n")
        slide_title = lines[0].replace("**", "").replace("Slide:", "").strip()
        slide = prs.slides.add_slide(bullet_slide_layout)
        slide.shapes.title.text = slide_title

        body_shape = slide.shapes.placeholders[1]
        tf = body_shape.text_frame
        tf.word_wrap = True

        for line in lines[1:]:
            clean_line = line.replace("*", "").strip()
            if clean_line:
                p = tf.add_paragraph()
                p.text = clean_line
                p.level = 0

    # Save to bytes
    output = BytesIO()
    prs.save(output)
    output.seek(0)
    return output


# ======================================================
# üîπ FastAPI Setup
# ======================================================
app = FastAPI(title="OpenManus Backend API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://open-manus-blue.vercel.app",
        "https://*.vercel.app",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"status": "ok", "message": "OpenManus backend is running."}

# ======================================================
# üîπ Request Model
# ======================================================
class PromptRequest(BaseModel):
    prompt: str


# ======================================================
# üîπ Helper for OpenRouter Calls
# ======================================================

def call_openrouter(prompt: str, system_instruction: str = None):
    """Calls OpenRouter Llama 3.3 70B (free or fallback) for smart text/code generation."""
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    if not openrouter_key:
        raise HTTPException(status_code=500, detail="Missing OPENROUTER_API_KEY")

    # Prepare message sequence
    messages = []
    if system_instruction:
        messages.append({"role": "system", "content": system_instruction})
    messages.append({"role": "user", "content": prompt})

    headers = {
        "Authorization": f"Bearer {openrouter_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://open-manus-blue.vercel.app",
        "X-Title": "NavaAI",
    }

    # Primary model
    models_to_try = [
        #"meta-llama/llama-3.3-70b-instruct:free",  # ‚úÖ new reliable free model
        #"mistralai/mixtral-8x7b",                  # fallback model
        "deepseek/deepseek-r1-distill-llama-70b"
    ]

    for model in models_to_try:
        payload = {"model": model, "messages": messages}
        print(f"üß† Trying OpenRouter model: {model}")
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            data=json.dumps(payload),
        )

        if response.status_code == 200:
            data = response.json()
            return data["choices"][0]["message"]["content"]

        print(f"‚ö†Ô∏è Model {model} failed ({response.status_code}): {response.text}")

    raise HTTPException(status_code=500, detail="All OpenRouter models failed.")

# ======================================================
# üîπ Main Unified Prompt Router
# ======================================================

@app.post("/api/run")
async def run_prompt(request: PromptRequest):
    """
    Unified intelligent route ‚Äî automatically detects intent and routes through agent service.
    Flow: Prompt ‚Üí api.py ‚Üí agent_service ‚Üí main.py (Manus agent)
    """
    prompt = request.prompt.strip()
    logger.info(f"üß† Received prompt: {prompt}")

    # ======================================================
    # üß† Route all requests through agent service (main.py flow)
    # ======================================================
    try:
        response = await agent_service.process_prompt(prompt)
        logger.info(f"‚úÖ Agent service completed with intent: {response.intent}")
        
        # Format response based on intent type
        if response.intent == "image":
            return {"type": "image", "output": response.output, "intent": response.intent}
        elif response.intent == "presentation":
            return {"type": "presentation", "output": response.output, "intent": response.intent}
        elif response.intent == "python":
            return {"type": "code", "output": response.output, "intent": response.intent}
        elif response.intent == "website":
            return {"type": "website", "output": response.output, "intent": response.intent}
        else:
            return {"type": "text", "output": response.output, "intent": response.intent}
            
    except Exception as e:
        logger.exception(f"‚ùå Agent service error: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

# ======================================================
# üîπ Code / PPT / Website Endpoints (Unchanged)
# ======================================================
class ImageRequest(BaseModel):
    prompt: str
    size: str = "1024x1024"

@app.post("/api/generate-image")
async def generate_image(req: ImageRequest):
    """Legacy image endpoint (uses ClipDrop)"""
    try:
        clipdrop_api_key = os.getenv("CLIPDROP_API_KEY")
        if not clipdrop_api_key:
            raise HTTPException(status_code=500, detail="Missing CLIPDROP_API_KEY")

        response = requests.post(
            "https://clipdrop-api.co/text-to-image/v1",
            files={"prompt": (None, req.prompt, "text/plain")},
            headers={"x-api-key": clipdrop_api_key},
            timeout=60,
        )

        if response.ok:
            image_bytes = response.content
            base64_image = base64.b64encode(image_bytes).decode("utf-8")
            image_url = f"data:image/png;base64,{base64_image}"
            return {"type": "image", "image_url": image_url}
        else:
            return {"type": "image", "image_url": "https://placehold.co/1024x1024?text=ClipDrop+Error"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")


@app.post("/api/generate-ppt")
async def generate_ppt(request: PromptRequest):
    try:
        response = await agent_service.process_prompt(request.prompt, intent="presentation")
        slides = [block.strip() for block in response.output.split("\n\n") if block.strip()]
        return {"type": "ppt", "slides": slides, "raw": response.output}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PPT generation failed: {str(e)}")


@app.post("/api/live-preview")
async def live_preview(request: PromptRequest):
    try:
        response = await agent_service.process_prompt(request.prompt, intent="website")
        html_content = response.output.strip()
        return JSONResponse({"type": "website", "html": html_content})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Website generation failed: {str(e)}")


@app.websocket("/ws/prompt")
async def prompt_websocket(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            payload = await websocket.receive_text()
            try:
                data = json.loads(payload)
            except json.JSONDecodeError:
                await websocket.send_json({"status": "error", "detail": "Invalid JSON payload."})
                continue

            prompt = (data.get("prompt") or "").strip()
            intent = data.get("intent")
            if not prompt:
                await websocket.send_json({"status": "error", "detail": "Prompt cannot be empty."})
                continue

            detected_intent = intent or agent_service.detect_intent(prompt)
            await websocket.send_json({"status": "processing", "intent": detected_intent})
            try:
                result = await agent_service.process_prompt(prompt, intent=detected_intent)
                await websocket.send_json(
                    {"status": "completed", "intent": result.intent, "output": result.output}
                )
            except Exception as exc:
                logger.exception("WebSocket processing failed.")
                await websocket.send_json({"status": "error", "detail": str(exc)})
    except WebSocketDisconnect:
        logger.info("WebSocket client disconnected.")
    except Exception as exc:
        logger.exception("Unexpected WebSocket error.")
        await websocket.close(code=1011, reason="Internal server error")

if __name__ == "__main__":
    import uvicorn, os
    port = int(os.getenv("PORT", 8000))
    uvicorn.run("api:app", host="0.0.0.0", port=port)